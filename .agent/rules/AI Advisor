# Rol: AI Advisor (LLM Local)

## 1. Nombre

**AI Advisor (LLM Local)** — `ai-advisor`

## 2. Misión

Interpretar la intención del jugador en lenguaje natural, explicar builds y recomendar configuraciones usando un LLM local (Ollama). NUNCA calcula stats directamente — delega al Build Engine para toda optimización numérica.

## 3. Alcance

### Incluye

- Cliente HTTP de Ollama (`src/lib/ai/client.ts`)
- AI Advisor service: system prompt, detección de intenciones, re-ranking (`src/lib/ai/service.ts`)
- Streaming de respuestas LLM (ReadableStream → text/plain)
- Route Handlers: `/api/ai/chat`, `/api/ai/status`
- Prompt engineering y mantenimiento del system prompt
- Fallback cuando Ollama no está disponible
- Parseo robusto de JSON del LLM (tolerante a formato irregular)

### Excluye

- Cálculo de stats y scoring de builds (→ Build Engine)
- Algoritmo de optimización combinatoria (→ Build Engine)
- Descarga de inventario (→ Backend)
- Gestión del Manifest (→ Manifest & Data Modeling)
- UI del chat (→ Frontend)

## 4. Entradas requeridas

| Artefacto | Fuente |
|-----------|--------|
| `optimizeArmor()` y tipos de builds | Build Engine |
| Inventario del jugador vía API | Backend |
| Definiciones del Manifest (items, perks, stats) | Manifest & Data Modeling |
| Variables de entorno: `OLLAMA_HOST`, `OLLAMA_MODEL` | Arquitectura |
| Especificación de intenciones y presets | Producto |

## 5. Salidas obligatorias

| Artefacto | Destino |
|-----------|---------|
| `src/lib/ai/client.ts` estable | Testing |
| `src/lib/ai/service.ts` estable | Testing |
| Endpoints `/api/ai/chat` y `/api/ai/status` | Frontend |
| Spec de streaming protocol (format I/O) | Frontend |
| `docs/handoffs/ai-advisor_<yyyymmdd-hhmm>.md` | Orquestador |

## 6. Ownership

```
src/lib/ai/client.ts
src/lib/ai/service.ts
src/app/api/ai/chat/route.ts
src/app/api/ai/status/route.ts
```

## 7. Interfaces

### Produce

| Interfaz | Tipo | Consumida por |
|----------|------|---------------|
| `POST /api/ai/chat` | REST streaming (text/plain) | Frontend |
| `GET /api/ai/status` | REST JSON `{ online, model }` | Frontend |
| `understandGoal(message)` | TS function (server) | Internal |
| `explainBuild(build)` | TS function (server) | Internal |

### Consume

| Interfaz | Tipo | Producida por |
|----------|------|---------------|
| `optimizeArmor(items, config)` | TS function | Build Engine |
| `GET /api/inventory` (server-side) | REST JSON | Backend |
| `getManifestDefinition()` | TS function | Manifest |
| Ollama API (`/api/chat`, `/api/tags`) | HTTP NDJSON | Ollama (externo) |
| Session service | TS module | Backend |

### Formatos I/O clave

```typescript
// POST /api/ai/chat — Request
interface AIChatRequest {
  messages: { role: 'user' | 'assistant'; content: string }[];
  stream?: boolean;  // default true
}

// Response: text/plain stream (chunked)
// Cada chunk es texto plano que se concatena en el cliente

// GET /api/ai/status — Response
interface AIStatusResponse {
  online: boolean;
  model: string | null;
  error?: string;
}
```

## 8. Guardrails

- ❌ **NO** calcular stats o tiers — delegar SIEMPRE al Build Engine
- ❌ **NO** exponer configuración de Ollama (host/model) al cliente
- ❌ **NO** enviar datos sensibles del jugador (tokens, IDs) al LLM
- ⚠️ System prompt actualizado con el meta actual de D2
- ⚠️ Parseo tolerante de JSON del LLM (puede devolver formato irregular)
- ⚠️ Timeout de 60s para respuestas del LLM
- ⚠️ Fallback obligatorio cuando Ollama está offline: mensaje claro, no crash
- ⚠️ Streaming chunked para UX responsiva (no esperar respuesta completa)

## 9. Definition of Done

- [ ] `/api/ai/chat` hace streaming fluido de respuestas
- [ ] `/api/ai/status` reporta correctamente online/offline + modelo
- [ ] AI Advisor detecta intenciones: pedir build, explicar item, preguntar meta
- [ ] Integración con Build Engine funcional (delega cálculos)
- [ ] Fallback funciona cuando Ollama está offline
- [ ] System prompt produce respuestas coherentes sobre D2
- [ ] No se filtran datos sensibles al LLM
- [ ] `npm run build` sin errores
- [ ] Handoff generado en `docs/handoffs/`

## 10. Formato de handoff

> Archivo: `docs/handoffs/ai-advisor_<yyyymmdd-hhmm>.md`

### Resumen

Estado del advisor: capacidades, modelo usado, calidad de respuestas.

### Cambios

```
- src/lib/ai/service.ts (nuevo/modificado)
- src/app/api/ai/chat/route.ts (nuevo)
- src/app/api/ai/status/route.ts (nuevo)
```

### Cómo probar

```bash
# Asegurarse de que Ollama está corriendo:
ollama serve
ollama pull llama3

npm run dev

# Test status:
curl https://localhost:3000/api/ai/status

# Test chat: navegar a /advisor en el browser
# O via curl:
curl -X POST https://localhost:3000/api/ai/chat \
  -H 'Content-Type: application/json' \
  -d '{"messages":[{"role":"user","content":"Build me a resilience titan"}]}'
```

### Decisiones tomadas

- Ej: "Se usa streaming en vez de esperar respuesta completa para UX"

### Riesgos / limitaciones

- Ej: "Sin GPU, la generación tarda 10-30s por mensaje"
- Ej: "Calidad depende del modelo instalado en Ollama"

### TODOs para el siguiente rol

- Ej: "Frontend debe implementar UI de streaming con indicador de typing"
- Ej: "Testing debe verificar fallback con Ollama apagado"
