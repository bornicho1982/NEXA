# Handoff: R7 AI Advisor

**Fecha**: 2026-02-16
**Rol**: R7 AI Advisor
**Estado**: ✅ Completado (MVP)

## Resumen Ejecutivo

Asistente de IA integrado mediante Ollama (Local LLM). El sistema utiliza RAG (Retrieval Augmented Generation) básico para inyectar contexto del Manifiesto de Destiny 2 en las conversaciones, permitiendo a la IA responder preguntas sobre items específicos.

## Entregables Técnicos

### 1. Cliente IA (`src/lib/ai/client.ts`)

- Conexión con Ollama (`http://localhost:11434`).
- Modelo configurable via `OLLAMA_MODEL` (default: `llama3`).

### 2. RAG System (`src/lib/ai/rag.ts`)

- Búsqueda semántica (substring search simple para MVP) en el Manifest.
- Formatea descripciones de items para el Prompt del sistema.

### 3. API Chat (`/api/ai/chat`)

- Endpoint POST.
- Recibe historial de mensajes.
- Inyecta Prompt de Sistema + Contexto RAG.
- Devuelve respuesta JSON.

## Notas para Frontend (R8)

- El endpoint devuelve JSON `{ message: { content: "..." } }`. No es stream `text/event-stream` en este MVP.
- Frontend debe mostrar "Thinking..." mientras espera (Ollama en CPU puede tardar 2-5s).

## Siguientes Pasos (Handoff a R8)

- **R8 Frontend** debe implementar la interfaz de chat (burbujas, input, scroll).
- Se recomienda usar `useChat` de Vercel AI SDK (adaptado a fetch standard) o simple `useState`.
